{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf, SparkFiles\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper1(line):\n",
    "    punc = string.punctuation    \n",
    "    tmp, i, j, value = '', '', '', ''\n",
    "    flag = -1\n",
    "    \n",
    "    for word in line:\n",
    "        if word.isdigit():\n",
    "            tmp += word\n",
    "        elif word in punc:\n",
    "            if flag == 0:\n",
    "                i = tmp\n",
    "            elif flag == 1:\n",
    "                j = tmp\n",
    "            flag +=1\n",
    "            tmp = ''\n",
    "            \n",
    "    value = tmp\n",
    "\n",
    "    if line[0] == 'M':\n",
    "        return j, (line[0], i, value)\n",
    "    else:\n",
    "        return i, (line[0], j, value)\n",
    "    \n",
    "    \n",
    "def mapper2(line):\n",
    "    i = int(line[1][0][1])\n",
    "    j = int(line[1][1][1])\n",
    "    mul = int(line[1][0][2]) * int(line[1][1][2])\n",
    "    return (i, j), mul\n",
    "\n",
    "def reducer(a, b):\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    path = \"500input.txt\"\n",
    "    file = sc.textFile(path)\n",
    "    M = file.filter(lambda x: 'M' in x)\n",
    "    N = file.filter(lambda x: 'N' in x)\n",
    "\n",
    "    M1 = M.map(mapper1)\n",
    "    N1 = N.map(mapper1)\n",
    "    M1 = M1.join(N1)\n",
    "\n",
    "\n",
    "    Cal = M1.map(mapper2)\n",
    "    Ans = Cal.reduceByKey(reducer).sortByKey().collect()\n",
    "    #for a in Ans:\n",
    "        #print(a)\n",
    "\n",
    "    outfile = open('Outputfile.txt','w')\n",
    "    for a in Ans:\n",
    "        s = str(a[0][0]) + ',' + str(a[0][1]) + ',' + str(a[1]) + '\\n'\n",
    "        outfile.write(s)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDA_HW1 Matrix Multiplication\n",
    "\n",
    "在這次的作業中，我們要對500X500的大矩陣使用MapReduce的技巧來做矩陣乘法。\n",
    "\n",
    "對於乘法的結果矩陣 R，每一項的算式如下，其中i的大小是M矩陣的row大小，j則是N矩陣的column大小。\n",
    "\n",
    "$r_{ij} = \\sum m_{ik}n_{kj}$\n",
    "\n",
    "讀進檔案時，先用RDD的運算—filter將M、N兩個矩陣分開，方便之後的計算。\n",
    "\n",
    "我總共使用了兩個mapper function和一個reducer funcition，其中mapper1會分別對兩個矩陣的資料做，做完之後就會將兩筆資料join在一起，繼續往下做。\n",
    "\n",
    "    \n",
    "* <font color = blue size=4> mapper1 </font>\n",
    " \n",
    "    在mapper1中，首先先將從txt檔讀出來的資料整理成我們所希望的樣子：\n",
    "    $(key, value) = (k, (M, i, m_{ik}))$  或  $((k, (N, j, m_{kj}))$\n",
    "    \n",
    "    因為不能用list還有和list相關的運算，所以很土法煉鋼的把讀進來的string分成數字。\n",
    "    \n",
    "    分數字的方法是，在逗號之前的數字（char）都會是同一個數字（int），比如說12在string中會分別被讀成1和2兩個char，但我們要的是12。因此用$isdigit$判斷讀到的字是不是數字，若是的話就和之前讀到的串在一起，直到遇到逗號分隔，把這個串好的字保留下來。用flag判斷現在取出的數字是row/column index或是value。\n",
    "    \n",
    "    分別獲得矩陣名稱、index和該位置的值後，就能將這些值以我們要的key-value方式回傳做map。\n",
    "    \n",
    "    \n",
    "* <font color = blue size=4> mapper2 </font>\n",
    "\n",
    "    在將兩筆資料join $（ M.join(N) )$ 起來後，key值相同的項就會被放到一起，資料的型態會變成像這樣：\n",
    "    $(key, (value, value) )$，且對於同樣的key值會有$i * j$ 項（行）。\n",
    "    \n",
    "    其中每個value都是一個tuple：\n",
    "    $(k, (M, i, m_{ik}))$ 或 $(k, (N, j, n_{kj}))$\n",
    "    這樣對於每一項（行），我們就可以取出兩個矩陣中的兩個值做相乘 $m_{ik}n_{kj}$。\n",
    "    最後把$(i, j)$當作key，相乘的結果 $m_{ik}n_{kj}$ 當作value組成新的pair（為了最後的排序，三個數字都轉成int型別，最後要寫入檔案時再改回），做第二次的map。\n",
    "          \n",
    "          \n",
    "* <font color = blue size=4> reducer </font>  \n",
    "\n",
    "    經過mapper2之後資料的型態為 $ ((i, j), value) $，這時只要使用$reduceByKey$，並將reducer function定義為相加，將相同key的value加起來，就能獲得$r_{ij}$了！\n",
    "    \n",
    "\n",
    "    \n",
    "做完reduce後，順序可能不會像原本一樣由小到大排得好好的，所以先用$sortByKey()$對資料先做排序，最後用$collect()$把RDD轉換成list，再將裡面的數字轉換成字串，寫到txt檔中。\n",
    "    \n",
    "    \n",
    "#### Reference:\n",
    "* https://www.tutorialspoint.com/pyspark/pyspark_rdd.htm\n",
    "* https://www.learncodewithmike.com/2019/12/python-lambda-functions.html\n",
    "* https://codertw.com/%E7%A8%8B%E5%BC%8F%E8%AA%9E%E8%A8%80/711947/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
