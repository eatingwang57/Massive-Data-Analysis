{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf, SparkFiles\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDA_HW4 Locality-Sensitive Hashing (LSH)\n",
    "\n",
    "這次的作業要實作Locality-Sensitive Hashing，有三個大步驟，以不同的mapper、reducer function實作：\n",
    "\n",
    "\n",
    "### 1. Shingling\n",
    "    會跑for迴圈一一讀入所有的檔案，mapper1~3的操作都是在for迴圈中進行（也就是在同一個檔案內進行操作）。\n",
    "    \n",
    "   * <font color=blue size=4> mapper1 </font> \n",
    "   \n",
    "       mapper1會讀進檔案中的句子，並以空格作為字的分隔，再把所有的單字集中在一起。數字和標點符號的部分也會和英文字母節合成為一個單字，並沒有特別分開或去掉。把每一段的字都集中成一個list，作為key-value pair的value，為了以後reduce方便將key都設成同樣的字（\"key\"）。\n",
    "       \n",
    "       \n",
    "   * <font color=blue size=4> reduce </font>\n",
    "   \n",
    "   這次使用的reduce function比較簡潔，因此我沒有特別寫reducer，直接以lambda定義在主要的code中。\n",
    "   做完mapper1之後，會用reduceByKey把檔案中所有的字都reduce到同一個list中。\n",
    "   \n",
    "   \n",
    "   * <font color=blue size=4> mapper2 </font>\n",
    "   \n",
    "   在mapper2中把檔案中的字建成3-shingles，並以空格把字接在一起，最後回傳全部的shingles。\n",
    "   \n",
    "   \n",
    "   * <font color=blue size=4> mapper3 </font> \n",
    "   \n",
    "   mapper3中會把檔案編號和檔案內的shingles放在一起，形成（ 檔案編號，[shingle0, shingle1, ...] ）這樣的key-value pair。把shingle放入的同時，會檢查list中有沒有放過相同的shingle，如果重複的話就不需要放入了。  \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper1(line):\n",
    "    newLine = line.split(\" \")\n",
    "    wordList = []\n",
    "    for word in newLine:\n",
    "        if word != \"\":\n",
    "            wordList.append((\"key\", [word]))\n",
    "    return wordList\n",
    "\n",
    "\n",
    "def mapper2(line):\n",
    "    shingles = []\n",
    "    tmp = []\n",
    "    for i in range(len(line[1])-2):\n",
    "        tmp = line[1][i:i + 3]\n",
    "        tmp = \" \".join(tmp)\n",
    "        shingles.append(tmp)\n",
    "        tmp = []\n",
    "       \n",
    "    return shingles\n",
    "\n",
    "def mapper3(line):\n",
    "    ShinglesInADoc = []\n",
    "    for shingle in line:\n",
    "        if shingle not in ShinglesInADoc:\n",
    "            ShinglesInADoc.append(shingle)\n",
    "            \n",
    "    # (DocNum, [shingle0, shingle1, ... ])\n",
    "    return DocNum, ShinglesInADoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllShingles = []\n",
    "ShinglesInDocs = []\n",
    "\n",
    "DocNum = 0\n",
    "\n",
    "for i in range(1, 102):\n",
    "    if(i<10):\n",
    "        path = \"00\" + str(i) + \".txt\"\n",
    "    elif(i >= 10 and i <= 99):\n",
    "        path = \"0\"+ str(i) + \".txt\"\n",
    "    else:\n",
    "        path = str(i) + \".txt\"\n",
    "    \n",
    "    file = sc.textFile(path)\n",
    "    \n",
    "    # Shingles\n",
    "    DocNum= i\n",
    "    shingles = file.flatMap(mapper1).reduceByKey(lambda a, b: a+b).map(mapper2)  \n",
    "    shingles = shingles.map(mapper3)\n",
    "    \n",
    "    # Min-Hashing Prepare\n",
    "    tmp = shingles.take(1)\n",
    "\n",
    "    for s in tmp[0][1]:\n",
    "        if s not in AllShingles:\n",
    "            AllShingles.append(s)\n",
    "    ShinglesInDocs.append(tmp[0])\n",
    "    \n",
    "shingles = sc.parallelize(ShinglesInDocs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Min-hashing\n",
    "\n",
    "    做完以上的部分之後，會為min hashing的步驟做一些前處理，這些處理也是在for迴圈裡面完成。\n",
    "    有一個list--AllShingles，用來存放所有的shingles，後面hash時會使用shingles在list中的index來做（用數字來做hash！）。因此在每個檔案做完mapper3後，會把產生的shingles放到AllShingles裡，同時也檢查有沒有相同的已經存在在其中，如果重複的話就不用再放了。\n",
    "    還有另一個list--ShinglesInDocs，會在迴圈的最後把經由mapper3處理完的、每個檔案的資料append進去，出迴圈後會將它轉為RDD的資料結構，在後面操作時使用。\n",
    "    \n",
    "   * <font color=blue size=4> mapper4 </font>\n",
    "   \n",
    "   從mapper4後就是在for迴圈外、對於所有檔案做處理的部分。\n",
    "   在mapper4中會將檔案中的shingles從英文字轉為數字，所使用到的數字就是該shingle在AllShingles中的index，並以相同的結構回傳。\n",
    "   \n",
    "   \n",
    "   * <font color=blue size=4> mapper5 </font>\n",
    "   \n",
    "   mapper5實做了min-hasning的主要部分。\n",
    "   我寫了兩個function用來產生random hash function，TestPrime用來找到用來做mod的質數，RandomCoef用來產生隨機的係數，根據傳進function的參數來產生不同範圍和數量的係數。\n",
    "   在mapper5就會使用這些係數來構成hash function，算式為$((a*shingle+b) \\%p) \\%N$，N為總共的shingle數量。總共有100個hash function，會拿所有的shingle去hash並用變數記住目前最小的計算結果，如果新hash出的值比它還小的話，就會更新。\n",
    "   最後回傳的結構和mapper3、mapper4的結構長得很相似，key為檔案的編號，value為100個min-hashing值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomCoef(num, size):\n",
    "    if(num == 1):\n",
    "        return random.randint(1, size)\n",
    "    else:\n",
    "        randomList = []\n",
    "        while num > 0:\n",
    "            r = random.randint(1, size)\n",
    "\n",
    "            # Ensure that each random number is unique.\n",
    "            while r in randomList:\n",
    "                r = random.randint(1, size)\n",
    "            randomList.append(r)\n",
    "            num-=1\n",
    "        return randomList\n",
    "    \n",
    "\n",
    "def TestPrime(num):\n",
    "    for i in range(2, num):\n",
    "        if(num%i == 0):\n",
    "            return False\n",
    "        else:\n",
    "            if(i == num-1): \n",
    "                return True\n",
    "            else: \n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper4(line):\n",
    "    indexList = []\n",
    "    for shingle in line[1]:\n",
    "        index = AllShingles.index(shingle)\n",
    "        indexList.append(index)\n",
    "    return line[0], indexList\n",
    "\n",
    "\n",
    "up = 1\n",
    "N = len(AllShingles)\n",
    "\n",
    "while not TestPrime(N + up):\n",
    "    up+=1\n",
    "    \n",
    "p = N + up\n",
    "a = RandomCoef(100, 300)\n",
    "b = RandomCoef(100, 300)\n",
    "\n",
    "\n",
    "def mapper5(line):\n",
    "    value = N\n",
    "    valueOpt = [N]*100 \n",
    "    for i in range(100):\n",
    "        for shingle in line[1]:\n",
    "            value = ((a[i] * shingle + b[i]) % p) % N\n",
    "            if(value < valueOpt[i]):\n",
    "                valueOpt[i] = value\n",
    "                \n",
    "    # (index, [mh0, mh1, mh2, ...])\n",
    "    return line[0], valueOpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min-Hashing\n",
    "shingles = shingles.map(mapper4)\n",
    "shingles_MH = shingles.map(mapper5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Locality-sensity hashing\n",
    "    接著是LSH的部分。\n",
    "    首先創建一個新的list--hashMatrix用來存所有檔案做完minhashing的值（也就是做完mapper5的資料，value部分），最後計算similarity時會使用。\n",
    "   \n",
    "   * <font color=blue size=4> mapper6 </font>\n",
    "   \n",
    "   在mapper6中會將資料分成50個band，每個band有兩個row，來做hash。一樣使用RandomCoef來產生係數，hash function的算式為$(a*row0 ＋b*row1 + c) \\% k$，k為自訂的bucket數量。最後會把資料建構成key為（band編號，算出來的bucket數），value為檔案編號構成的list的資料結構。\n",
    "   \n",
    "做完mapper6之後會做一次reduceByKey，把相同key的資料reduce在一起。使用的reducer一樣比較簡潔，也是使用lambda function。\n",
    "   * <font color=blue size=4> mapper7 </font>\n",
    "   \n",
    "   要成為Candidate pair的條件是他們要至少hash到同樣的bucket中至少一個band，在mapper7中會做篩選。篩選的方法是檢查value中檔案的數量，如果只有一個的話，就只會是一個空的list。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSH\n",
    "hashMatrix = [0] * 101\n",
    "tmp = shingles_MH.take(101)\n",
    "for doc in tmp:\n",
    "    hashMatrix[doc[0]-1] = doc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = RandomCoef(1, 10)\n",
    "b1 = RandomCoef(1, 10)\n",
    "c1 = RandomCoef(1, 10)\n",
    "\n",
    "def mapper6(line):\n",
    "    band = 50\n",
    "    row = 2\n",
    "    tmp = []\n",
    "    for i in range(band):\n",
    "        bucket = (a1 * line[1][i*row] + b1 * line[1][i*row+1] + c1) % 31\n",
    "        tmp.append(((i+1, bucket), [line[0]]))\n",
    "        \n",
    "    # [((band, bucket), doc), ...]\n",
    "    return tmp  \n",
    "\n",
    "# if there is only one doc, ignore it\n",
    "def mapper7(line):\n",
    "    tmp = []\n",
    "    if len(line[1]) != 1:\n",
    "        tmp.append((line))\n",
    "    \n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = shingles_MH.flatMap(mapper6).reduceByKey(lambda a, b: a+b).flatMap(mapper7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * <font color=blue size=4> mapper8 </font>\n",
    "    \n",
    "    接著是計算similarity的部分。\n",
    "    兩個document一組計算similarity，用剛剛建好的hashMatrix找，如果能被同一個hash function hash到同樣的值，就將count加一。最後similarity的計算方式就是count除以總has function數（100）。回傳的資料結構為（（檔案編號0, 檔案編號1）, similarity）。\n",
    "    \n",
    "    \n",
    "因為相同的檔案pair可能有重複出現好幾次，我們只要保留一次就好，因此再用一個reduceByKey，reducer function同樣為lambda funciton，回傳a就好。\n",
    "\n",
    "   * <font color=blue size=4> mapper9 </font>\n",
    "   \n",
    "    最後將資料調整成（ similarity, （檔案編號0, 檔案編號1））的結構，以方便後面以similarity排序，找出最大的前十個。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper8(line):\n",
    "    tmp = []\n",
    "    cnt = 0\n",
    "    for i in range(len(line[1])):\n",
    "        for j in range(len(line[1])):\n",
    "            if i < j:\n",
    "                for idx in range(100):\n",
    "                    vi = hashMatrix[line[1][i]-1][idx]\n",
    "                    vj = hashMatrix[line[1][j]-1][idx]\n",
    "                    if vi == vj: \n",
    "                        cnt+=1\n",
    "                tmp.append([(line[1][i], line[1][j]), cnt/100])\n",
    "                cnt = 0\n",
    "    return tmp\n",
    "\n",
    "def mapper9(line):\n",
    "    return (line[1], line[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "\n",
      "(52, 84) : 1.0\n",
      "(12, 20) : 1.0\n",
      "(30, 35) : 0.8\n",
      "(47, 49) : 0.72\n",
      "(23, 38) : 0.53\n",
      "(49, 88) : 0.53\n",
      "(48, 49) : 0.48\n",
      "(14, 40) : 0.42\n",
      "(47, 88) : 0.39\n",
      "(47, 48) : 0.34\n"
     ]
    }
   ],
   "source": [
    "# Similarity\n",
    "Sim = bands.flatMap(mapper8).reduceByKey(lambda a, b: a)\n",
    "Result = Sim.map(mapper9).sortByKey(ascending= False)\n",
    "\n",
    "# Output\n",
    "outfile = open('Outputfile.txt','w')\n",
    "outfile.write(\"Output:\\n\")\n",
    "\n",
    "print(\"Output:\\n\")\n",
    "Output = Result.take(10)\n",
    "for line in Output:\n",
    "    print(line[1], \":\", line[0])\n",
    "    outfile.write(str(line[1]) + \": \" + str(line[0]) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
